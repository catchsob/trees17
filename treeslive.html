<title>Trees TF.js</title>
<div id="select">Image: <input type="file" id="select_img" accept="image/*, capture=camera" onchange="preview()"></div>
<div>Prediction: <label id="pred"></label>/ Confidence: <label id="conf"></label></div>
<input id="livebutton" type="button" value="live" onclick="golive()">
<div id="live"></div>
<br>
<img id="preview_img" /><br>

<style type="text/css">
    div, input {
        font-size: 4vw;
    }
    label {
        color: brown;
    }
    img {
        width: 100%;
    }
</style>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.0/dist/tf.min.js"></script>
<script>
    const MODEL_NAME = 'YOUR_MODEL_TFJS';
    const ENV = 'env.json'
    const LABELS_NAME = 'YOUR_LABELS';
    const IMAGE_SIZE = 448;
    let env;
    let model = null;
    let labels;
	let video;
	let canvas;
	let cam;
	let liveapp;
	let info = 'loading...';
	let infocolor = 'black';
    let live = false;
	let liveview;
	
	function golive() {
		live = !live;
		if (live) {
			document.getElementById("preview_img").style.visibility = 'hidden';
			initlive();
		}
		else {
			liveapp.removeChild(liveview);
			liveview = null;
			if (video.src.active) {
				video.src.stop();
			}
			video = null;
			cnavas = null;
			document.getElementById("preview_img").style.visibility = 'visible';
		}
	}
	
    function preview() {
        if (!window.FileReader) {
            console.log('no preview functionality supported by your browser!');
            return;
        }
        
        let reader = new FileReader();
        reader.onload = function (event) {
            let img = document.getElementById("preview_img");
            img.src = event.target.result;
            predict(img);
        };

        let file = document.getElementById("select_img").files[0];
        reader.readAsDataURL(file);
	}
	
	async function init() {
		console.log('start ...')
        env = load_env(ENV)
		//model = await tf.loadGraphModel(env[MODEL_NAME]);
		//model.predict(tf.zeros([1, IMAGE_SIZE, IMAGE_SIZE, 3])).dispose();
		//console.log('model ' + env[MODEL_NAME] + ' loaded.')
        labels = load_labels(env[LABELS_NAME]).split("\r\n")
        console.log('labels ' + env[LABELS_NAME] + ' loaded.')
	}
    
    async function predict(imgElement) {
		model = await tf.loadGraphModel(env[MODEL_NAME]);
		//console.log('model ' + env[MODEL_NAME] + ' loaded.')
		
        const logits = tf.tidy(() => {
            const img = tf.cast(tf.browser.fromPixels(imgElement).resizeBilinear([IMAGE_SIZE, IMAGE_SIZE]), 'float32');
            const offset = tf.scalar(255.0);
            const normalized = img.div(offset);
            const batched = normalized.reshape([1, IMAGE_SIZE, IMAGE_SIZE, 3]);
            return model.predict(batched);
        });
        
        const values = await logits.data();
        let p = -1;
        let v = -0.1;
        for (let i = 0; i < values.length; i++) {
            if (values[i] > v) {
                v = values[i];
                p = i
            }
        }
        document.getElementById("pred").innerHTML = labels[p]
		info = labels[p]
        document.getElementById("conf").innerHTML = v.toPrecision(4)
		if (v > 0.9) {
			infocolor = "green";
		}
		else if (v > 0.7) {
			infocolor = "yellow";
		}
		else if (v > 0.5) {
			infocolor = "red";
		}
		else {
			infocolor = "black";
		}
        console.log({value: v, index: p, label: labels[p]});
    }
    
	function load_file(filePath) {
        var result = null;
        var xmlhttp = new XMLHttpRequest();
        xmlhttp.open("GET", filePath, false);
        xmlhttp.send();
        if (xmlhttp.status==200) {
            result = xmlhttp.responseText;
        }
        return result;
    }
	
	function load_labels(filePath) {
        return load_file(filePath);
    }
	
	function load_env(filePath) {
		return JSON.parse(load_file(filePath));
    }
    
	//init().then(()=>{
	//   console.log('init finished!')
	//});

    const constraints = {
      audio: false,
      video: {
        facingMode: "environment" // user or environment
      }
    };
    
    const getFrameFromVideo = (video, canvas) => {
	    if (!live) {
			return;
		}
        const ctx = canvas.getContext("2d");
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        ctx.save();
        ctx.drawImage(video, 0, 0, video.width, video.height);
        predict(video);
        ctx.font = "80px serif";
	    ctx.fillStyle = infocolor;
        ctx.fillText(info, 50, 100);
        ctx.restore();
        requestAnimationFrame(() => getFrameFromVideo(video, canvas));
    };
    
    const getCameraStream = video => {
      navigator.mediaDevices
        .getUserMedia(constraints)
        .then(function success(stream) {
          video.srcObject = stream;
		  cam = stream;
        });
    };
    
    const createVideo = (id, width, height) => {
      const video = document.createElement("video");
      video.id = id;
      video.width = width;
      video.height = height;
      video.autoplay = true;
      //video.controls = true;
      return video;
    };
    
    const createCanvas = (id, width, height) => {
      const canvas = document.createElement("canvas");
      canvas.id = id;
      canvas.width = width;
      canvas.height = height;
      return canvas;
    };
    
    const initlive = () => {
		console.log('initializing live ...')
        video = createVideo("vid", 800, 800);
        canvas = createCanvas("canvas", 800, 800);
        liveapp = document.getElementById("live");
        getCameraStream(video);
        getFrameFromVideo(video, canvas);
        //app.appendChild(video);
        liveview = liveapp.appendChild(canvas);
        console.log("live initialized.");
    };
    
    document.getElementById("select").onload = init();
</script>